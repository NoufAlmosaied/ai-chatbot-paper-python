# Configuration for Phase 2: Data Annotation and Preprocessing Pipeline

# Dataset Configuration
datasets:
  email_dataset: "data/raw/phishing_email.csv"
  url_dataset: "data/raw/Phishing_Legitimate_full.csv"
  max_samples: null  # Use null for all samples, or specify a number for testing

# Automated Tagging Configuration
tagging:
  apply_tagging: true
  batch_size: 100
  
  # Keywords for pattern detection
  urgency_keywords:
    - urgent
    - immediate
    - act now
    - expire
    - suspend
    - deadline
    - limited time
    - hurry
    
  threat_keywords:
    - suspend
    - terminate
    - close
    - block
    - lock
    - disable
    - deactivate
    
  financial_keywords:
    - payment
    - invoice
    - refund
    - credit
    - debit
    - account
    - transaction
    - money

# Preprocessing Configuration
preprocessing:
  # Data splitting
  test_size: 0.15
  val_size: 0.15
  random_state: 42
  
  # TF-IDF parameters
  tfidf_max_features: 5000
  tfidf_ngram_range: [1, 3]
  tfidf_min_df: 5
  tfidf_max_df: 0.95
  
  # Data augmentation
  apply_smote: true
  
  # Feature normalization
  normalize_features: true

# Validation Configuration
validation:
  generate_report: true
  create_visualizations: true
  
  # Quality thresholds
  outlier_threshold: 3.0
  correlation_threshold: 0.9
  
  # Validation checks
  check_missing: true
  check_duplicates: true
  check_distribution: true
  check_outliers: true
  check_correlation: true

# Output Configuration
output:
  processed_dir: "data/processed"
  reports_dir: "data/reports"
  save_intermediate: true
  
  # File formats
  save_format: "npy"  # Options: npy, csv, pickle
  compress: false

# Expert Review Configuration
expert_review:
  sample_size: 100
  stratified: true
  save_reviews: true
  review_output: "data/processed/expert_reviews.json"

# Performance Configuration
performance:
  n_jobs: -1  # Use all CPU cores
  verbose: true
  memory_limit: "8GB"